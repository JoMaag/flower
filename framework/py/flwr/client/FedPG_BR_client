from flwr.client import NumPyClient
import numpy as np

from flwr.common import (
    Config,
    NDArrays,
    Scalar,
    ndarrays_to_parameters,
    parameters_to_ndarrays,
)

class FedPGClient(NumPyClient):
    def __init__(self, env, policy_network, cid: str):
        self.env = env
        self.policy = policy_network
        self.cid = cid

    def fit(self, parameters, config):
        """
        Client only does lines 5-6:
        - Sample B_t trajectories
        - Compute gradient μ^{(k)}_t
        """
        theta_t0 = parameters[0]
        Bt = config["Bt"]
        
        # Line 5: Sample trajectories
        trajectories = [
            self._sample_trajectory(theta_t0) 
            for _ in range(Bt)
        ]
        
        # Line 6: Compute gradient
        mu_k = self._compute_batch_gradient(trajectories, theta_t0)
        
        # Return gradient to server
        return [mu_k], len(trajectories), {}
    
    def _sample_trajectory(self, theta):
        """Sample trajectory from client's MDP."""
        self.policy.set_weights(theta)
        
        states, actions, rewards = [], [], []
        state = self.env.reset()
        done = False
        
        while not done:
            action = self.policy.sample_action(state)
            next_state, reward, done, _ = self.env.step(action)
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            state = next_state
        
        return {'states': states, 'actions': actions, 'rewards': rewards}
    
    def _compute_batch_gradient(self, trajectories, theta):
        """μ = (1/B_t) Σ g(τ_i|θ)."""
        grads = [self._compute_gradient(tau, theta) for tau in trajectories]
        return np.mean(grads, axis=0)
    
    def _compute_gradient(self, trajectory, theta):
        """REINFORCE gradient estimator."""
        states = trajectory['states']
        actions = trajectory['actions']
        rewards = trajectory['rewards']
        
        # Returns
        gamma = 0.99
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        
        # Gradient
        grad = np.zeros_like(theta)
        for s, a, G in zip(states, actions, returns):
            grad += self.policy.log_prob_gradient(s, a, theta) * G
        
        return grad / len(states)