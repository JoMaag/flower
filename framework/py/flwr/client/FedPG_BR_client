from flwr.client import NumPyClient
import numpy as np

from flwr.common import (
    Config,
    NDArrays,
    Scalar,
    ndarrays_to_parameters,
    parameters_to_ndarrays,
)

class FedPGClient(NumPyClient):
    """
    Minimal template for a FedPG-BR RL client.

    Users should implement:
    - sample_trajectory(): sample a trajectory from their environment
    - compute_gradient(): compute policy gradient for a trajectory
    """

    def __init__(self, policy):
        """
        Args:
            policy: Your RL policy object. Must have get_weights() and set_weights().
        """
        self.policy = policy

    def get_parameters(self, config: dict[str, Scalar]) -> list[np.ndarray]:
        """
        Return the current local model parameters.

        Parameters
        ----------
        config : dict[str, Scalar]
            Configuration parameters requested by the server. Can be ignored or used
            to decide which parameters to return.

        Returns
        -------
        parameters : list[np.ndarray]
            The local model parameters as a list of NumPy arrays.
        """
        # Ignore config if not needed
        _ = config

        # Return current policy parameters as a list of NumPy arrays
        return [self.policy.get_weights()]

    def fit(self, parameters, config):
        """
        FedPG-BR client update.

        Args:
            parameters: Global parameters θ₀ᵗ (list of NumPy arrays)
            config: Dict containing "Bt" (batch size) and optionally other hyperparams

        Returns:
            - Updated gradient μₜ^(k) as list of NumPy arrays
            - Number of trajectories used
            - Optional metrics dict
        """
        # Load global parameters
        self.policy.set_weights(parameters[0])

        Bt = int(config.get("Bt", 32))

        # 1. Sample Bt trajectories
        trajectories = [self.sample_trajectory() for _ in range(Bt)]

        # 2. Compute gradients for each trajectory
        grads = [self.compute_gradient(tau) for tau in trajectories]

        # 3. Average to get μₜ^(k)
        mu_k = np.mean(grads, axis=0)

        # Return as Flower expects: list of ndarrays
        return [mu_k], len(trajectories), {}

    def evaluate(self, parameters, config):
        """Optional evaluation; can return dummy values"""
        return 0.0, 0, {}

    # ----------------------
    # User must implement these
    # ----------------------
    def sample_trajectory(self):
        """
        Sample a trajectory from your environment using the current policy.
        Must return a trajectory object suitable for compute_gradient().
        """
        raise NotImplementedError("Users must implement sample_trajectory()")

    def compute_gradient(self, trajectory):
        """
        Compute policy gradient (REINFORCE/GPOMDP) for a single trajectory.
        Must return a NumPy array representing the gradient.
        """
        raise NotImplementedError("Users must implement compute_gradient()")
