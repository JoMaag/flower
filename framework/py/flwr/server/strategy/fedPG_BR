from .strategy import Strategy
from flwr.common import EvaluateIns, EvaluateRes, FitIns, FitRes, Parameters, Scalar, ndarrays_to_parameters, parameters_to_ndarrays
from flwr.server.client_manager import ClientManager
from flwr.server.client_proxy import ClientProxy
from typing import Optional, Union, List, Dict
import numpy as np


class fedPG_BR(Strategy):
    def __init__(
        self,
        init_params: np.ndarray,
        env,  # ← SERVER'S MDP
        policy_network,  # ← SERVER'S POLICY
        batch_size: int,
        minibatch_size: int,
        step_size: float,
        sigma: float = 1.0,
        alpha: float = 0.3,
        delta: float = 0.1
    ):
        self.theta_tilde = init_params
        self.env = env  # Server's MDP copy
        self.policy = policy_network
        self.Bt = batch_size
        self.bt = minibatch_size
        self.eta_t = step_size
        self.sigma = sigma
        self.alpha = alpha
        self.delta = delta
        self.mu_t = None

    def sample_trajectory(self, theta: np.ndarray):
        """
        Sample trajectory from SERVER'S MDP using policy θ.
        """
        self.policy.set_weights(theta)
        
        states, actions, rewards = [], [], []
        state = self.env.reset()
        done = False
        
        while not done:
            action = self.policy.sample_action(state)
            next_state, reward, done, _ = self.env.step(action)
            
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            state = next_state
        
        return {
            'states': states,
            'actions': actions,
            'rewards': rewards
        }
    
    def compute_gradient(self, trajectory, theta: np.ndarray) -> np.ndarray:
        """
        REINFORCE gradient: g(τ|θ) = Σ_t [∇log π(a_t|s_t)] * G_t
        """
        states = trajectory['states']
        actions = trajectory['actions']
        rewards = trajectory['rewards']
        
        # Compute returns (discounted cumulative rewards)
        gamma = 0.99  # discount factor
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        
        # Policy gradient
        grad = np.zeros_like(theta)
        for s, a, G in zip(states, actions, returns):
            # ∇_θ log π_θ(a|s)
            grad += self.policy.log_prob_gradient(s, a, theta) * G
        
        return grad / len(states)  # normalize


    def initialize_parameters(self, client_manager) -> Optional[Parameters]:
        """Return initial theta_0 as Flower Parameters.

        Parameters
        ----------
        client_manager : ClientManager
            The client manager which holds all currently connected clients.

        Returns
        -------
        parameters : Optional[Parameters]
            If parameters are returned, then the server will treat these as the
            initial global model parameters.
        """
        return ndarrays_to_parameters([self.theta_tilde])
    
    def configure_fit(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> list[tuple[ClientProxy, FitIns]]:
        """Configure the next round of training."""
        
        clients = list(client_manager.all().values())

        # Clients need only B_t (batch size)
        fit_ins = FitIns(
            parameters=parameters,  # θ^t_0
            config={
                "Bt": self.Bt,  # Clients sample B_t trajectories
            }
        )
        
        return [(client, fit_ins) for client in clients]


    def aggregate_fit(
        self,
        server_round: int,
        results: list[tuple[ClientProxy, FitRes]],
        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],
    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:
        """Aggregate training results."""
        
        if not results:
            return None, {}

        # Step 1: Collect gradients from clients (Line 6)
        gradients = []
        for _, fit_res in results:
            grad = parameters_to_ndarrays(fit_res.parameters)[0]
            gradients.append(grad)

        # Step 2: Byzantine-resilient aggregation (Line 7)
        self.mu_t = self.fedpg_aggregate(gradients)

        # Step 3: Server performs inner loop (Lines 8-12)
        self.theta_tilde = self.inner_update()

        # Step 4: Metrics
        metrics: Dict[str, Scalar] = {
            "num_clients": len(results),
            "num_failures": len(failures),
            "mu_t_norm": float(np.linalg.norm(self.mu_t)),
            "theta_norm": float(np.linalg.norm(self.theta_tilde)),
        }

        # Step 5: Return updated global model
        return ndarrays_to_parameters([self.theta_tilde]), metrics

    # -------------------------
    # FedPG-Aggregate (server-side)
    # -------------------------
    def fedpg_aggregate(self, grads: List[np.ndarray]) -> np.ndarray:
        """
        Algorithm 1.1: FedPG-Aggregate
        Byzantine-resilient gradient aggregation.
        """
        K = len(grads)
        V = 2 * np.log(2 * K / self.delta)
        T_mu = 2 * self.sigma * np.sqrt(V / self.Bt)

        # Try R1 first (lines 2-4)
        S1 = self._build_vector_median_set(grads, T_mu)
        
        if S1:
            mu_mom = self._find_mean_of_median(S1)
            Gt = [g for g in grads if np.linalg.norm(g - mu_mom) <= T_mu]
        else:
            Gt = []

        # Fallback to R2 if needed (lines 5-8)
        if len(Gt) < (1 - self.alpha) * K:
            S2 = self._build_vector_median_set(grads, 2 * self.sigma)
            
            if S2:
                mu_mom = self._find_mean_of_median(S2)
                Gt = [g for g in grads if np.linalg.norm(g - mu_mom) <= 2 * self.sigma]
            else:
                # Emergency: use all (shouldn't happen if α < 0.5)
                Gt = grads

        # Line 9: Return mean of "good" gradients
        if not Gt:
            Gt = grads
            
        return np.mean(Gt, axis=0)


    def _build_vector_median_set(self, grads: List[np.ndarray], threshold: float) -> List[np.ndarray]:
        """
        S = {μ_k : |{k' : ||μ_k' - μ_k|| <= threshold}| > K/2}
        
        Selects gradients that have more than K/2 neighbors within threshold.
        """
        K = len(grads)
        S = []
        
        for g in grads:
            # Count neighbors within threshold
            neighbors = sum(
                1 for g2 in grads 
                if np.linalg.norm(g - g2) <= threshold
            )
            
            if neighbors > K / 2:
                S.append(g)
        
        return S


    def _find_mean_of_median(self, S: List[np.ndarray]) -> np.ndarray:
        """
        μ_mom ← argmin_{μ ∈ S} ||μ - mean(S)||
        
        Returns the element in S closest to the mean of S.
        """
        if not S:
            raise ValueError("S is empty!")
            
        mean_S = np.mean(S, axis=0)
        return min(S, key=lambda g: np.linalg.norm(g - mean_S))
        
    def inner_update(self) -> np.ndarray:
        """
        SCSG Inner Loop (Lines 8-12).
        Server samples trajectories from its MDP.
        """
        # Line 8: Sample number of iterations
        p = self.Bt / (self.Bt + self.bt)
        Nt = np.random.geometric(p)

        theta_tn = self.theta_tilde.copy()  # θ^t_0

        for n in range(Nt):
            # Line 10: Sample b_t trajectories from SERVER's MDP
            # Key: Use theta_tn (changes each iteration)
            mini_trajectories = [
                self.sample_trajectory(theta_tn) 
                for _ in range(self.bt)
            ]

            # Line 11: Variance-reduced gradient (Equation 3)
            corrections = []
            for tau in mini_trajectories:
                # Current gradient
                g_n = self.compute_gradient(tau, theta_tn)
                
                # Reference gradient (importance-weighted)
                g_0 = self.compute_gradient(tau, self.theta_tilde)
                omega = self.compute_omega(tau, theta_tn, self.theta_tilde)
                
                corrections.append(g_n - omega * g_0)
            
            correction_term = np.mean(corrections, axis=0)
            v_tn = correction_term + self.mu_t

            # Line 12: Update
            theta_tn = theta_tn + self.eta_t * v_tn

        return theta_tn  # θ̃_t


    def compute_omega(self, trajectory, theta_new: np.ndarray, theta_old: np.ndarray) -> float:
        """
        Importance weight: ω(τ|θ_new, θ_old) = p(τ|θ_old) / p(τ|θ_new)
        
        Args:
            trajectory: dict with 'states', 'actions'
            theta_new: Policy that generated the trajectory
            theta_old: Reference policy
        """
        states = trajectory['states']
        actions = trajectory['actions']
        
        # Compute log probabilities
        log_prob_old = sum(
            self.policy.log_prob(s, a, theta_old)
            for s, a in zip(states, actions)
        )
        log_prob_new = sum(
            self.policy.log_prob(s, a, theta_new)
            for s, a in zip(states, actions)
        )
        
        # Importance weight (with clipping for numerical stability)
        log_omega = log_prob_old - log_prob_new
        log_omega = np.clip(log_omega, -10, 10)  # prevent overflow
        
        return np.exp(log_omega)

    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> list[tuple[ClientProxy, EvaluateIns]]:
        """Configure the next round of evaluation.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        parameters : Parameters
            The current (global) model parameters.
        client_manager : ClientManager
            The client manager which holds all currently connected clients.

        Returns
        -------
        evaluate_configuration : List[Tuple[ClientProxy, EvaluateIns]]
            A list of tuples. Each tuple in the list identifies a `ClientProxy` and the
            `EvaluateIns` for this particular `ClientProxy`. If a particular
            `ClientProxy` is not included in this list, it means that this
            `ClientProxy` will not participate in the next round of federated
            evaluation.
        """
        return []


    def aggregate_evaluate(
        self,
        server_round: int,
        results: list[tuple[ClientProxy, EvaluateRes]],
        failures: list[Union[tuple[ClientProxy, EvaluateRes], BaseException]],
    ) -> tuple[Optional[float], dict[str, Scalar]]:
        """Aggregate evaluation results.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        results : List[Tuple[ClientProxy, FitRes]]
            Successful updates from the
            previously selected and configured clients. Each pair of
            `(ClientProxy, FitRes` constitutes a successful update from one of the
            previously selected clients. Not that not all previously selected
            clients are necessarily included in this list: a client might drop out
            and not submit a result. For each client that did not submit an update,
            there should be an `Exception` in `failures`.
        failures : List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]]
            Exceptions that occurred while the server was waiting for client updates.

        Returns
        -------
        aggregation_result : Tuple[Optional[float], Dict[str, Scalar]]
            The aggregated evaluation result. Aggregation typically uses some variant
            of a weighted average.
        """
        return None, {}


    def evaluate(
        self, server_round: int, parameters: Parameters
    ) -> Optional[tuple[float, dict[str, Scalar]]]:
        """Evaluate the current model parameters.

        This function can be used to perform centralized (i.e., server-side) evaluation
        of model parameters.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        parameters: Parameters
            The current (global) model parameters.

        Returns
        -------
        evaluation_result : Optional[Tuple[float, Dict[str, Scalar]]]
            The evaluation result, usually a Tuple containing loss and a
            dictionary containing task-specific metrics (e.g., accuracy).
        """
        return None
