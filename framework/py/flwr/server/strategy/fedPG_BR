from .strategy import Strategy
from flwr.common import EvaluateIns, EvaluateRes, FitIns, FitRes, Parameters, Scalar, ndarrays_to_parameters, parameters_to_ndarrays
from flwr.server.client_manager import ClientManager
from flwr.server.client_proxy import ClientProxy
from typing import Optional, Union, List, Dict
import numpy as np


class fedPG_BR(Strategy):
    def __init__(self, init_params: np.ndarray, batch_size: int, minibatch_size: int, step_size: float):
        # Global policy θ̃₀
        self.theta_tilde = init_params
        self.Bt = batch_size
        self.bt = minibatch_size
        self.eta_t = step_size
        self.mu_t = None
        self.all_thetas = []


    def initialize_parameters(self, client_manager) -> Optional[Parameters]:
        """Return initial theta_0 as Flower Parameters."""


        """Initialize the (global) model parameters.

        Parameters
        ----------
        client_manager : ClientManager
            The client manager which holds all currently connected clients.

        Returns
        -------
        parameters : Optional[Parameters]
            If parameters are returned, then the server will treat these as the
            initial global model parameters.
        """
        return ndarrays_to_parameters([self.theta_tilde])
        

    def configure_fit(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> list[tuple[ClientProxy, FitIns]]:
        """Configure the next round of training.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        parameters : Parameters
            The current (global) model parameters.
        client_manager : ClientManager
            The client manager which holds all currently connected clients.

        Returns
        -------
        fit_configuration : List[Tuple[ClientProxy, FitIns]]
            A list of tuples. Each tuple in the list identifies a `ClientProxy` and the
            `FitIns` for this particular `ClientProxy`. If a particular `ClientProxy`
            is not included in this list, it means that this `ClientProxy`
            will not participate in the next round of federated learning.
        """
        clients = client_manager.all().values()  # returns dict[ClientProxy]

        fit_ins = FitIns(
            parameters=parameters,  # global θ₀ᵗ
            config={
                "Bt": self.Bt,  # batch size per client
                "bt": self.bt   # mini-batch size for inner updates
            }
        )

        # Return list of tuples: (ClientProxy, FitIns)
        return [(client, fit_ins) for client in clients]


    def aggregate_fit(
        self,
        server_round: int,
        results: list[tuple[ClientProxy, FitRes]],
        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],
    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:
        """Aggregate training results.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        results : List[Tuple[ClientProxy, FitRes]]
            Successful updates from the previously selected and configured
            clients. Each pair of `(ClientProxy, FitRes)` constitutes a
            successful update from one of the previously selected clients. Not
            that not all previously selected clients are necessarily included in
            this list: a client might drop out and not submit a result. For each
            client that did not submit an update, there should be an `Exception`
            in `failures`.
        failures : List[Union[Tuple[ClientProxy, FitRes], BaseException]]
            Exceptions that occurred while the server was waiting for client
            updates.

        Returns
        -------
        parameters : Tuple[Optional[Parameters], Dict[str, Scalar]]
            If parameters are returned, then the server will treat these as the
            new global model parameters (i.e., it will replace the previous
            parameters with the ones returned from this method). If `None` is
            returned (e.g., because there were only failures and no viable
            results) then the server will no update the previous model
            parameters, the updates received in this round are discarded, and
            the global model parameters remain the same.
        """
        # 1. Collect gradients from clients (successful FitRes only)
        gradients = []
        for _, fit_res in results:
            # Each client returns μₜ^(k) as a numpy array (Parameters -> list of ndarrays)
            grad = parameters_to_ndarrays(fit_res.parameters)[0]
            gradients.append(np.array(grad))

        if not gradients:
            # If no gradients received, skip update
            return None, {}

        # 2. Byzantine-resilient aggregation (Algorithm 1.1)
        self.mu_t = self.fedpg_aggregate(gradients)

        # 3. Inner variance-reduced update loop (lines 8–12)
        self.theta_tilde = self.inner_update()

        # 4. Optional: store metrics
        metrics: Dict[str, Scalar] = {
            "num_clients": len(results),
            "num_failures": len(failures),
        }

        # 5. Return updated global model as Flower Parameters
        return ndarrays_to_parameters([self.theta_tilde]), metrics

    # -------------------------
    # FedPG-Aggregate (server-side)
    # -------------------------
    def fedpg_aggregate(self, grads: List[np.ndarray]) -> np.ndarray:
        """
        Implements Algorithm 1.1: Byzantine-resilient gradient aggregation.
        """
        sigma = 1.0
        delta = 0.1
        Bt = self.Bt
        K = len(grads)

        T_mu = 2 * sigma * np.sqrt(np.log(2*K/delta) / Bt)

        # Step 2: S1
        S1 = [g for g in grads if np.linalg.norm(g - np.mean(grads, axis=0)) <= T_mu]
        if len(S1) == 0:
            S1 = grads

        mu_mom = min(S1, key=lambda g: np.linalg.norm(g - np.mean(S1, axis=0)))
        Gt = [g for g in grads if np.linalg.norm(g - mu_mom) <= T_mu]
        if len(Gt) == 0:
            Gt = grads

        # Step 9: return mean of "non-Byzantine" gradients
        return np.mean(Gt, axis=0)
    
    def inner_update(self) -> np.ndarray:
        """
        Perform the inner variance-reduced updates for θ̃ₜ.

        Algorithm lines 8-12:
            8: Sample N_t ~ Geom(Bt / (Bt + bt))
            9-12: For n = 0..N_t-1, sample mini-batch of trajectories,
                  compute variance-reduced gradient, update theta_t_n.
        """
        # Line 8: sample number of inner iterations
        p = self.Bt / (self.Bt + self.bt)
        Nt = np.random.geometric(p)

        theta_t_n = self.theta_tilde.copy()

        for n in range(Nt):  # Lines 9-12
            # 1. Sample mini-batch of b_t trajectories
            trajectories = [self.sample_trajectory(theta_t_n) for _ in range(self.bt)]

            # 2. Compute variance-reduced gradient
            grads_curr = [self.compute_gradient(tau, theta_t_n) for tau in trajectories]
            grads_ref = [self.compute_gradient(tau, self.theta_tilde) for tau in trajectories]
            omega = [self.compute_omega(tau, theta_t_n, self.theta_tilde) for tau in trajectories]

            # Variance-reduced gradient: g_current - ω * g_ref + μ_t
            correction = np.mean([g_c - w * g_r for g_c, w, g_r in zip(grads_curr, omega, grads_ref)], axis=0)
            v_t_n = correction + self.mu_t

            # 3. Update theta_t_n
            theta_t_n = theta_t_n + self.eta_t * v_t_n

        # Return the updated θ̃ₜ after N_t inner updates
        return theta_t_n
    
    def sample_trajectory(self, theta: np.ndarray):
        """
        Sample a trajectory from the environment using policy theta.
        Must be implemented for your traffic RL setup.
        """
        raise NotImplementedError

    def compute_gradient(self, trajectory, theta: np.ndarray) -> np.ndarray:
        """
        Compute policy gradient (REINFORCE or GPOMDP) for a trajectory.
        """
        raise NotImplementedError

    def compute_omega(self, trajectory, theta_new: np.ndarray, theta_old: np.ndarray) -> float:
        """
        Importance weight for variance reduction.
        """
        return 1.0


    
    
    
    
    
    
    
    
    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> list[tuple[ClientProxy, EvaluateIns]]:
        """Configure the next round of evaluation.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        parameters : Parameters
            The current (global) model parameters.
        client_manager : ClientManager
            The client manager which holds all currently connected clients.

        Returns
        -------
        evaluate_configuration : List[Tuple[ClientProxy, EvaluateIns]]
            A list of tuples. Each tuple in the list identifies a `ClientProxy` and the
            `EvaluateIns` for this particular `ClientProxy`. If a particular
            `ClientProxy` is not included in this list, it means that this
            `ClientProxy` will not participate in the next round of federated
            evaluation.
        """
        return []


    def aggregate_evaluate(
        self,
        server_round: int,
        results: list[tuple[ClientProxy, EvaluateRes]],
        failures: list[Union[tuple[ClientProxy, EvaluateRes], BaseException]],
    ) -> tuple[Optional[float], dict[str, Scalar]]:
        """Aggregate evaluation results.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        results : List[Tuple[ClientProxy, FitRes]]
            Successful updates from the
            previously selected and configured clients. Each pair of
            `(ClientProxy, FitRes` constitutes a successful update from one of the
            previously selected clients. Not that not all previously selected
            clients are necessarily included in this list: a client might drop out
            and not submit a result. For each client that did not submit an update,
            there should be an `Exception` in `failures`.
        failures : List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]]
            Exceptions that occurred while the server was waiting for client updates.

        Returns
        -------
        aggregation_result : Tuple[Optional[float], Dict[str, Scalar]]
            The aggregated evaluation result. Aggregation typically uses some variant
            of a weighted average.
        """
        return None, {}


    def evaluate(
        self, server_round: int, parameters: Parameters
    ) -> Optional[tuple[float, dict[str, Scalar]]]:
        """Evaluate the current model parameters.

        This function can be used to perform centralized (i.e., server-side) evaluation
        of model parameters.

        Parameters
        ----------
        server_round : int
            The current round of federated learning.
        parameters: Parameters
            The current (global) model parameters.

        Returns
        -------
        evaluation_result : Optional[Tuple[float, Dict[str, Scalar]]]
            The evaluation result, usually a Tuple containing loss and a
            dictionary containing task-specific metrics (e.g., accuracy).
        """
        return None
